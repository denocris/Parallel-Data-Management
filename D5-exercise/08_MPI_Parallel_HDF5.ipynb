{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPI Parallel HDF5\n",
    "\n",
    "**Source:** *Python and HDF5* by Andrew Collette, O'Reilly 2013.\n",
    "\n",
    "We would like to share a single file among multiple processes and have some magic for synchronizing reads and writes.\n",
    "\n",
    "<img src=\"./img/MPI.png\" width=600 />\n",
    "\n",
    "### `mpi4py`\n",
    "\n",
    "#### `demo.py`\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "print \"Hello World (from process %d)\" % comm.rank\n",
    "```\n",
    "\n",
    "### Run\n",
    "\n",
    "```shell\n",
    "$ mpiexec -n 4 python demo.py\n",
    "Hello World (from process 0)\n",
    "Hello World (from process 1)\n",
    "Hello World (from process 3)\n",
    "Hello World (from process 2)\n",
    "```\n",
    "\n",
    "### Use the MPI/IO-based VFD\n",
    "\n",
    "For this to work, you'll need a version of the HDF5 library with the MPI-parallel extensions enabled. Unfortunately, none of the mainstream Python distros ship with \n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "import h5py\n",
    "\n",
    "f = h5py.File(\"foo.hdf5\", \"w\", driver=\"mpio\", comm=MPI.COMM_WORLD)\n",
    "```\n",
    "\n",
    "#### Our Favorite Example\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import h5py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD   # Communicator which links all our processes together\n",
    "rank = comm.rank        # Number which identifies this process.  Since we'll\n",
    "                        # have 4 processes, this will be in the range 0-3.\n",
    "\n",
    "f = h5py.File('coords.hdf5', driver='mpio', comm=comm)\n",
    "\n",
    "coords_dset = f['coords']\n",
    "distances_dset = f.create_dataset('distances', (1000,), dtype='f4')\n",
    "\n",
    "idx = rank*250  # This will be our starting index.  Rank 0 handles coordinate\n",
    "                # pairs 0-249, Rank 1 handles 250-499, Rank 2 500-749, and\n",
    "                # Rank 3 handles 750-999.\n",
    "\n",
    "coords = coords_dset[idx:idx+250]  # Load process-specific data\n",
    "\n",
    "result = np.sqrt(np.sum(coords**2, axis=1))  # Compute distances\n",
    "\n",
    "distances_dset[idx:idx+250] = result  # Write process-specific data\n",
    "\n",
    "f.close()\n",
    "```\n",
    "\n",
    "### Atomicity\n",
    "\n",
    "Without the line `f.atomic = True` all bets are off as to what will be printed.\n",
    "\n",
    "```python\n",
    "import h5py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "\n",
    "with h5py.File('atomicdemo.hdf5', 'w', driver='mpio', comm=comm) as f:\n",
    "\n",
    "    f.atomic = True  # Enables strict atomic mode (requires HDF5 1.8.9+)\n",
    "\n",
    "    dset = f.create_dataset('x', (1,), dtype='i')\n",
    "\n",
    "    if rank == 0:\n",
    "        dset[0] = 42\n",
    "\n",
    "    comm.Barrier()\n",
    "\n",
    "    if rank == 1:\n",
    "        print dset[0]\n",
    "```\n",
    "\n",
    "## Topics for Discussion\n",
    "\n",
    "- Building the HDF5 library with parallel extensions enabled\n",
    "  + `CC=/usr/local/mpi/bin/mpicc ./configure --enable-parallel --prefix=<install-directory>`\n",
    "- Building `h5py` with parallel extensions enabled\n",
    "  + `python setup.py build --mpi [--hdf5=/path/to/parallel/hdf5]`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
